# Phase 4: Learning Infrastructure

Build feedback loops for continuous improvement. **Zero runtime latency impact**.

<pre>
â”œâ”€â”€ <a href="./README.md">&lt;&lt;merc&gt;&gt;</a>
â”œâ”€â”€ <a href="./scoring-algorithm.md">Scoring Algorithm</a>
â”œâ”€â”€ <a href="./1.foundation.md">1. Foundation</a>
â”œâ”€â”€ <a href="./2.labels.md">2. Label Expansion</a>
â”œâ”€â”€ <a href="./3.context.md">3. Context & Ensemble</a>
â”œâ”€â”€ <a href="./4.learning.md"><b>4. Learning Infrastructure</b></a> ðŸ‘ˆ
â””â”€â”€ <a href="./5.output.md">5. Output Enrichment</a>
</pre>

---

## Overview

| ID | Task | Runtime Impact | Purpose | Status |
|----|------|----------------|---------|--------|
| MERC-010 | Feedback Logging | <1ms | Capture decisions for review | ðŸ”² Not Started |
| MERC-011 | Benchmark Dataset | 0% | Measure accuracy | âœ… Complete |
| MERC-012 | Weight Tuning Pipeline | 0% | Continuous improvement | ðŸ”² Not Started |

**Goal:** Enable data-driven weight tuning without impacting runtime performance.

---

## MERC-010: Feedback Logging

**Status:** ðŸ”² Not Started

### Problem

No mechanism to learn from mistakes. The same errors repeat without any visibility into patterns.

### Solution

Log scoring decisions for periodic review and weight tuning.

```mermaid
flowchart TD
    A[Score Decision] --> B[Log to Storage]
    B --> C[Periodic Review]
    C --> D{Correct?}

    D -->|Yes| E[Reinforce weights]
    D -->|No| F[Adjust weights]

    E --> G[Updated Parameters]
    F --> G
```

### Implementation

```rust
use chrono::{DateTime, Utc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FeedbackLog {
    pub id: Uuid,
    pub text: String,
    pub context: Option<String>,
    pub score: f32,
    pub label_scores: Vec<LabelScore>,
    pub decision: Decision,
    pub confidence: Confidence,
    pub timestamp: DateTime<Utc>,
    // Added later by review
    pub correct: Option<bool>,
    pub notes: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Decision {
    Accept,
    Reject,
}
```

### Storage Options

| Option | Pros | Cons |
|--------|------|------|
| File (JSON/CSV) | Simple, portable | No querying |
| SQLite | Local, queryable | Single process |
| PostgreSQL | Scalable, queryable | Requires setup |

MVP: File-based storage (JSON lines)

```rust
pub trait FeedbackStore {
    fn log(&self, entry: FeedbackLog) -> Result<()>;
    fn get_unlabeled(&self) -> Result<Vec<FeedbackLog>>;
    fn mark_correct(&self, id: Uuid, correct: bool) -> Result<()>;
}

// File-based implementation
pub struct JsonLinesStore {
    path: PathBuf,
}
```

### Files

- `src/score/feedback.rs` (new)
- `src/score/mod.rs` â€” Add logging call

### Tasks

- [ ] Create `FeedbackLog` struct
- [ ] Create `FeedbackStore` trait
- [ ] Implement `JsonLinesStore` (MVP)
- [ ] Add logging to scoring pipeline
- [ ] Create CLI for reviewing logs

### Acceptance Criteria

- Decisions logged with <1ms overhead
- Pluggable storage backend
- Review workflow documented

---

## MERC-011: Benchmark Dataset

**Status:** âœ… Complete

### Problem

No standardized way to measure accuracy changes. Improvements are "felt" rather than measured.

### Solution

Created a curated benchmark dataset with ground-truth labels and a CLI for running benchmarks.

### Dataset Structure

```json
{
  "version": "1.0.0",
  "created": "2026-02-02",
  "samples": [
    {
      "id": "task-001",
      "text": "I need to finish the quarterly report by end of day",
      "context": null,
      "expected_decision": "accept",
      "expected_labels": ["task", "time"],
      "primary_category": "task",
      "difficulty": "easy"
    },
    {
      "id": "phatic-001",
      "text": "Hey, how's it going?",
      "context": null,
      "expected_decision": "reject",
      "expected_labels": ["phatic"],
      "primary_category": "phatic",
      "difficulty": "easy"
    }
  ]
}
```

### Categories

| Category | Description | Current Count |
|----------|-------------|---------------|
| Task | Clear tasks/commitments | 17 |
| Emotional | Emotion-laden content | 15 |
| Factual | Facts/information | 15 |
| Preference | Preferences/opinions | 15 |
| Decision | Decisions/choices | 25 |
| Phatic | Small talk (reject) | 15 |
| Ambiguous | Edge cases | 10 |
| **Total** | | **112** |

### CLI Usage

```bash
# Validate dataset
cargo run --package merc-cli -- bench validate libs/merc-engine/benches/dataset.json

# Run benchmark
cargo run --package merc-cli -- bench run libs/merc-engine/benches/dataset.json

# Check label coverage
cargo run --package merc-cli -- bench coverage libs/merc-engine/benches/dataset.json
```

### Benchmark Output

```
=== Benchmark Results ===

Total samples: 112
Correct:       X (X.X%)

Precision: X.XXX
Recall:    X.XXX
F1 Score:  X.XXX

=== Per-Category Results ===
...

=== Per-Label Results ===
Label                Expect Detect     TP     Prec   Recall       F1
--------------------------------------------------------------------------
...
```

### Files

- `src/bench/mod.rs` â€” Module exports
- `src/bench/dataset.rs` â€” Dataset schema, validation, coverage
- `src/bench/runner.rs` â€” Benchmark runner with metrics
- `benches/dataset.json` â€” 112 labeled samples
- `libs/merc-cli/` â€” CLI crate with bench subcommand

### Tasks

- [x] Create dataset schema (`BenchDataset`, `BenchSample`)
- [x] Create validation logic (duplicate IDs, valid labels, valid decisions)
- [x] Write benchmark runner with accuracy, precision, recall, F1 metrics
- [x] Create CLI (`merc bench run/validate/coverage`)
- [x] Generate 112 labeled samples covering all 26 labels
- [ ] Add to CI pipeline â€” *Future work*
- [ ] Expand to 350+ samples â€” *Future work*

### Acceptance Criteria

- [x] 100+ labeled samples (112 created)
- [x] Automated benchmark runner
- [x] All 26 labels covered
- [ ] Accuracy tracked in CI â€” *Future work*

---

## MERC-012: Weight Tuning Pipeline

**Status:** ðŸ”² Not Started

### Problem

Weights are hand-tuned based on intuition. No systematic way to optimize.

### Solution

Use feedback logs and benchmark data to tune weights.

```mermaid
flowchart TD
    A[Feedback Logs] --> B[Analyze Errors]
    C[Benchmark Dataset] --> B

    B --> D{Error Pattern?}

    D -->|False Positive| E[Lower weight for triggering label]
    D -->|False Negative| F[Raise weight for missed label]
    D -->|Threshold issue| G[Adjust threshold]

    E --> H[Generate Weight Updates]
    F --> H
    G --> H

    H --> I[Test on Benchmark]
    I --> J{Improved?}

    J -->|Yes| K[Apply Updates]
    J -->|No| L[Revert]
```

### Analysis Script

```rust
fn analyze_errors(logs: &[FeedbackLog], benchmark: &Dataset) -> WeightRecommendations {
    let false_positives = logs.iter()
        .filter(|l| l.decision == Decision::Accept && l.correct == Some(false))
        .collect::<Vec<_>>();

    let false_negatives = logs.iter()
        .filter(|l| l.decision == Decision::Reject && l.correct == Some(false))
        .collect::<Vec<_>>();

    // Find patterns in errors
    let fp_patterns = find_common_labels(&false_positives);
    let fn_patterns = find_common_labels(&false_negatives);

    WeightRecommendations {
        decrease: fp_patterns,  // Labels causing false positives
        increase: fn_patterns,  // Labels missing from false negatives
    }
}
```

### Tuning Process

1. **Collect feedback** â€” Run in production, label errors
2. **Analyze patterns** â€” Find common labels in errors
3. **Generate recommendations** â€” Suggest weight changes
4. **Test on benchmark** â€” Verify improvement
5. **Deploy** â€” Update weights in config

### Files

- `tools/tune_weights.rs` (new)
- `config/weights.toml` â€” Externalize weights

### Tasks

- [ ] Externalize weights to config file
- [ ] Create analysis script
- [ ] Create tuning CLI
- [ ] Document tuning process
- [ ] Add safeguards (max adjustment per iteration)

### Acceptance Criteria

- Weights in external config (not code)
- Analysis script identifies error patterns
- Documented tuning workflow

---

## Research Context

### How Other Systems Learn

| System | Learning Approach |
|--------|-------------------|
| Merc (proposed) | Feedback logs â†’ weight tuning |
| Zep | No explicit learning (relies on LLM) |
| Hindsight | Opinion confidence updates with evidence |
| Enterprise Model | No explicit learning (fixed BERT) |

**Hindsight's CARA system** updates opinion confidence based on evidence:
- Strong support: `c' = c + Î±(1 - c)`
- Contradiction: `c' = c Ã— Î³`

Merc's approach is simplerâ€”tune static weights based on observed errorsâ€”but achieves a similar goal of continuous improvement.

---

## Workflow Summary

```mermaid
flowchart LR
    A[Production] --> B[Log Decisions]
    B --> C[Review Errors]
    C --> D[Analyze Patterns]
    D --> E[Tune Weights]
    E --> F[Test on Benchmark]
    F --> G[Deploy]
    G --> A
```

---

## Testing Requirements

- [ ] Unit tests for logging
- [x] Benchmark runner tests (11 tests in `src/bench/`)
- [ ] Weight tuning safeguards
- [ ] CI integration for benchmarks

---

## Next Phase

After completing Phase 4, proceed to [Phase 5: Output Enrichment](./5.output.md) for structured output format and downstream integration.
